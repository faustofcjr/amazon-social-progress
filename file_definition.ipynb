{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d15866-1711-4dda-802c-b4baf48fc763",
   "metadata": {},
   "source": [
    "# Caipora Project\n",
    "\n",
    "__Goal__\n",
    "\n",
    "To carry out projections of hotspots in Brazilian territory through public data collected by satellites and provided by INPE – Queimadas. and to prevent possible threats attractive from predictive models.\n",
    " \n",
    "__Data Source__\n",
    "\n",
    "https://queimadas.dgi.inpe.br/queimadas/portal\n",
    "\n",
    "https://openaq.org/#/\n",
    "\n",
    "https://ipsamazonia.org.br/\n",
    "\n",
    "\n",
    "__Data characteristics__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944fd5d-ca44-476e-85f4-3c7a91d5451a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e2b0c-4422-4483-b5fa-9a7719ee2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAZILIAN_STATES = [\n",
    "    { \"acronym\": \"AC\", \"name\": \"Acre\" },\n",
    "    { \"acronym\": \"AL\", \"name\": \"Alagoas\" },\n",
    "    { \"acronym\": \"AP\", \"name\": \"Amapá\" },\n",
    "    { \"acronym\": \"AM\", \"name\": \"Amazonas\" },\n",
    "    { \"acronym\": \"BA\", \"name\": \"Bahia\" },\n",
    "    { \"acronym\": \"CE\", \"name\": \"Ceará\" },\n",
    "    { \"acronym\": \"DF\", \"name\": \"Distrito Federal\" },\n",
    "    { \"acronym\": \"ES\", \"name\": \"Espírito Santo\" },\n",
    "    { \"acronym\": \"GO\", \"name\": \"Goiás\" },\n",
    "    { \"acronym\": \"MA\", \"name\": \"Maranhão\" },\n",
    "    { \"acronym\": \"MT\", \"name\": \"Mato Grosso\" },\n",
    "    { \"acronym\": \"MS\", \"name\": \"Mato Grosso do Sul\" },\n",
    "    { \"acronym\": \"MG\", \"name\": \"Minas Gerais\" },\n",
    "    { \"acronym\": \"PA\", \"name\": \"Pará\" },\n",
    "    { \"acronym\": \"PB\", \"name\": \"Paraíba\" },\n",
    "    { \"acronym\": \"PR\", \"name\": \"Paraná\" },\n",
    "    { \"acronym\": \"PE\", \"name\": \"Pernambuco\" },\n",
    "    { \"acronym\": \"PI\", \"name\": \"Piauí\" },\n",
    "    { \"acronym\": \"RJ\", \"name\": \"Rio de Janeiro\" },\n",
    "    { \"acronym\": \"RN\", \"name\": \"Rio Grande do Norte\" },\n",
    "    { \"acronym\": \"RS\", \"name\": \"Rio Grande do Sul\" },\n",
    "    { \"acronym\": \"RO\", \"name\": \"Rondônia\" },\n",
    "    { \"acronym\": \"RR\", \"name\": \"Roraima\" },\n",
    "    { \"acronym\": \"SC\", \"name\": \"Santa Catarina\" },\n",
    "    { \"acronym\": \"SP\", \"name\": \"São Paulo\" },\n",
    "    { \"acronym\": \"SE\", \"name\": \"Sergipe\" },\n",
    "    { \"acronym\": \"TO\", \"name\": \"Tocantins\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ea1d7-37d8-457d-bce6-9a5a4ec6a2de",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991572e9-338a-419e-80fe-a5c7ac1a61c5",
   "metadata": {},
   "source": [
    "### Amazon SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f457f08e-3088-4cbd-9cc9-5f0a310487d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SPI_PATH = \"data/spi/amazonia\"\n",
    "with pd.ExcelFile(f\"{SPI_PATH}/ips_tabela_completa_modificada.xlsx\") as xlsx:\n",
    "    \n",
    "    for sheetname in xlsx.sheet_names:\n",
    "        # Recupera o ano a partir do nome da aba\n",
    "        match_years = re.findall(r'.*([1-3][0-9]{3})', sheetname)\n",
    "\n",
    "        if len(match_years) > 0:\n",
    "            select_year = int(match_years[0])\n",
    "            \n",
    "            # Carregar os dados releacionados a aba cujo nome esta referenciado pela variável sheetname\n",
    "            dataset = pd.read_excel(xlsx, sheetname)\n",
    "            dataset.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "            \n",
    "            # Adiciona a feature ano\n",
    "            dataset.insert(0, \"Ano\",select_year)\n",
    "            \n",
    "            # Padroniza o nome da coluna 3 retirando o ano do final\n",
    "            dataset.rename(columns={ dataset.columns[4]: \"IPS Amazônia\" }, inplace=True)\n",
    "            \n",
    "            # Cria um novo arquivo csv com o nome do ano presente em sheetname.\n",
    "            detailed_filepath = Path(f\"{SPI_PATH}/detailed/{select_year}.csv\")\n",
    "            detailed_filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "            dataset.to_csv(detailed_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fe0e0-9308-4e09-a7ac-5a744ecae68a",
   "metadata": {},
   "source": [
    "### Hotspot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884d6e5-f2e3-40fc-8ef1-9935a4654212",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"data/2012/2012-01.csv\",\n",
    "    \"data/2012/2012-02.csv\",\n",
    "    \"data/2012/2012-03.csv\",\n",
    "    \"data/2012/2012-04.csv\",\n",
    "    \"data/2012/2012-05.csv\",\n",
    "    \"data/2012/2012-06.csv\",\n",
    "    \"data/2012/2012-07.csv\",\n",
    "    \"data/2012/2012-08.csv\",\n",
    "    \"data/2012/2012-09.csv\",\n",
    "    \"data/2012/2012-10.csv\",\n",
    "    \"data/2012/2012-11.csv\",\n",
    "    \"data/2012/2012-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2012.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2013/2013-01.csv\",\n",
    "    \"data/2013/2013-02.csv\",\n",
    "    \"data/2013/2013-03.csv\",\n",
    "    \"data/2013/2013-04.csv\",\n",
    "    \"data/2013/2013-05.csv\",\n",
    "    \"data/2013/2013-06.csv\",\n",
    "    \"data/2013/2013-07.csv\",\n",
    "    \"data/2013/2013-08.csv\",\n",
    "    \"data/2013/2013-09.csv\",\n",
    "    \"data/2013/2013-10.csv\",\n",
    "    \"data/2013/2013-11.csv\",\n",
    "    \"data/2013/2013-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2013.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2014/2014-01.csv\",\n",
    "    \"data/2014/2014-02.csv\",\n",
    "    \"data/2014/2014-03.csv\",\n",
    "    \"data/2014/2014-04.csv\",\n",
    "    \"data/2014/2014-05.csv\",\n",
    "    \"data/2014/2014-06.csv\",\n",
    "    \"data/2014/2014-07.csv\",\n",
    "    \"data/2014/2014-08.csv\",\n",
    "    \"data/2014/2014-09.csv\",\n",
    "    \"data/2014/2014-10.csv\",\n",
    "    \"data/2014/2014-11.csv\",\n",
    "    \"data/2014/2014-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2014.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2015/2015-01.csv\",\n",
    "    \"data/2015/2015-02.csv\",\n",
    "    \"data/2015/2015-03.csv\",\n",
    "    \"data/2015/2015-04.csv\",\n",
    "    \"data/2015/2015-05.csv\",\n",
    "    \"data/2015/2015-06.csv\",\n",
    "    \"data/2015/2015-07.csv\",\n",
    "    \"data/2015/2015-08.csv\",\n",
    "    \"data/2015/2015-09.csv\",\n",
    "    \"data/2015/2015-10.csv\",\n",
    "    \"data/2015/2015-11.csv\",\n",
    "    \"data/2015/2015-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2015.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2016/2016-01.csv\",\n",
    "    \"data/2016/2016-02.csv\",\n",
    "    \"data/2016/2016-03.csv\",\n",
    "    \"data/2016/2016-04.csv\",\n",
    "    \"data/2016/2016-05.csv\",\n",
    "    \"data/2016/2016-06.csv\",\n",
    "    \"data/2016/2016-07.csv\",\n",
    "    \"data/2016/2016-08.csv\",\n",
    "    \"data/2016/2016-09.csv\",\n",
    "    \"data/2016/2016-10.csv\",\n",
    "    \"data/2016/2016-11.csv\",\n",
    "    \"data/2016/2016-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2016.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2017/2017-01.csv\",\n",
    "    \"data/2017/2017-02.csv\",\n",
    "    \"data/2017/2017-03.csv\",\n",
    "    \"data/2017/2017-04.csv\",\n",
    "    \"data/2017/2017-05.csv\",\n",
    "    \"data/2017/2017-06.csv\",\n",
    "    \"data/2017/2017-07.csv\",\n",
    "    \"data/2017/2017-08.csv\",\n",
    "    \"data/2017/2017-09.csv\",\n",
    "    \"data/2017/2017-10.csv\",\n",
    "    \"data/2017/2017-11.csv\",\n",
    "    \"data/2017/2017-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2017.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2018/2018-01.csv\",\n",
    "    \"data/2018/2018-02.csv\",\n",
    "    \"data/2018/2018-03.csv\",\n",
    "    \"data/2018/2018-04.csv\",\n",
    "    \"data/2018/2018-05.csv\",\n",
    "    \"data/2018/2018-06.csv\",\n",
    "    \"data/2018/2018-07.csv\",\n",
    "    \"data/2018/2018-08.csv\",\n",
    "    \"data/2018/2018-09.csv\",\n",
    "    \"data/2018/2018-10.csv\",\n",
    "    \"data/2018/2018-11.csv\",\n",
    "    \"data/2018/2018-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2018.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2019/2019-01.csv\",\n",
    "    \"data/2019/2019-02.csv\",\n",
    "    \"data/2019/2019-03.csv\",\n",
    "    \"data/2019/2019-04.csv\",\n",
    "    \"data/2019/2019-05.csv\",\n",
    "    \"data/2019/2019-06.csv\",\n",
    "    \"data/2019/2019-07.csv\",\n",
    "    \"data/2019/2019-08.csv\",\n",
    "    \"data/2019/2019-09.csv\",\n",
    "    \"data/2019/2019-10.csv\",\n",
    "    \"data/2019/2019-11.csv\",\n",
    "    \"data/2019/2019-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2019.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2020/2020-01.csv\",\n",
    "    \"data/2020/2020-02.csv\",\n",
    "    \"data/2020/2020-03.csv\",\n",
    "    \"data/2020/2020-04.csv\",\n",
    "    \"data/2020/2020-05.csv\",\n",
    "    \"data/2020/2020-06.csv\",\n",
    "    \"data/2020/2020-07.csv\",\n",
    "    \"data/2020/2020-08.csv\",\n",
    "    \"data/2020/2020-09.csv\",\n",
    "    \"data/2020/2020-10.csv\",\n",
    "    \"data/2020/2020-11.csv\",\n",
    "    \"data/2020/2020-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2020.csv\", index=False)\n",
    "\n",
    "files = [\n",
    "    \"data/2021/2021-01.csv\",\n",
    "    \"data/2021/2021-02.csv\",\n",
    "    \"data/2021/2021-03.csv\",\n",
    "    \"data/2021/2021-04.csv\",\n",
    "    \"data/2021/2021-05.csv\",\n",
    "    \"data/2021/2021-06.csv\",\n",
    "    \"data/2021/2021-07.csv\",\n",
    "    \"data/2021/2021-08.csv\",\n",
    "    \"data/2021/2021-09.csv\",\n",
    "    \"data/2021/2021-10.csv\",\n",
    "    \"data/2021/2021-11.csv\",\n",
    "    \"data/2021/2021-12.csv\",\n",
    "]\n",
    "dataset = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "dataset = dataset[(dataset.estado == \"TOCANTINS\")]\n",
    "dataset.to_csv(\"data/hotspot/bra/to/hotspots_2021.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
